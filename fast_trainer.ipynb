{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /home/ubuntu/miniconda3/envs/py39-34/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 114\n",
      "CUDA SETUP: Loading binary /home/ubuntu/miniconda3/envs/py39-34/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda114.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/py39-34/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:147: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {Path('/home/ubuntu/miniconda3/envs/py39-34/lib/libcudart.so.11.0'), Path('/home/ubuntu/miniconda3/envs/py39-34/lib/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n",
      "\u001b[32m2024-05-16 11:03:59.260\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mhftrainer.trainer.base\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m96\u001b[0m - \u001b[1mUsing imemoize in Jupyter notebook.\u001b[0m\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-05-16 11:03:59.424\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mhftrainer.trainer.base\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m167\u001b[0m - \u001b[34m\u001b[1mLoading model and tokenizer with provided arguments.\u001b[0m\n",
      "/home/ubuntu/miniconda3/envs/py39-34/lib/python3.9/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Arguments table: model_args\n",
      "╒════════════════════╤═══════════════════╕\n",
      "│ model_name_or_path │ Qwen/Qwen1.5-0.5B │\n",
      "╘════════════════════╧═══════════════════╛\n",
      "\n",
      "\n",
      "Arguments table: data_args\n",
      "╒═════════════════╤══════════════════════╕\n",
      "│ data_path       │ ../shepherd_data.pkl │\n",
      "├─────────────────┼──────────────────────┤\n",
      "│ eval_data_path  │                      │\n",
      "├─────────────────┼──────────────────────┤\n",
      "│ lazy_preprocess │ True                 │\n",
      "╘═════════════════╧══════════════════════╛\n",
      "\n",
      "\n",
      "Arguments table: training_args\n",
      "╒═════════════════════════════╤═════════════════════════════╕\n",
      "│ output_dir                  │ output/deepseek_for_process │\n",
      "├─────────────────────────────┼─────────────────────────────┤\n",
      "│ overwrite_output_dir        │ True                        │\n",
      "├─────────────────────────────┼─────────────────────────────┤\n",
      "│ num_train_epochs            │ 18                          │\n",
      "├─────────────────────────────┼─────────────────────────────┤\n",
      "│ per_device_train_batch_size │ 64                          │\n",
      "├─────────────────────────────┼─────────────────────────────┤\n",
      "│ per_device_eval_batch_size  │ 1                           │\n",
      "├─────────────────────────────┼─────────────────────────────┤\n",
      "│ gradient_accumulation_steps │ 1                           │\n",
      "├─────────────────────────────┼─────────────────────────────┤\n",
      "│ evaluation_strategy         │ epoch                       │\n",
      "├─────────────────────────────┼─────────────────────────────┤\n",
      "│ save_strategy               │ epoch                       │\n",
      "├─────────────────────────────┼─────────────────────────────┤\n",
      "│ save_steps                  │ 10                          │\n",
      "├─────────────────────────────┼─────────────────────────────┤\n",
      "│ save_total_limit            │ 1                           │\n",
      "├─────────────────────────────┼─────────────────────────────┤\n",
      "│ learning_rate               │ 1e-05                       │\n",
      "├─────────────────────────────┼─────────────────────────────┤\n",
      "│ weight_decay                │ 0.01                        │\n",
      "├─────────────────────────────┼─────────────────────────────┤\n",
      "│ adam_beta2                  │ 0.95                        │\n",
      "├─────────────────────────────┼─────────────────────────────┤\n",
      "│ warmup_ratio                │ 0.0                         │\n",
      "├─────────────────────────────┼─────────────────────────────┤\n",
      "│ lr_scheduler_type           │ cosine                      │\n",
      "├─────────────────────────────┼─────────────────────────────┤\n",
      "│ logging_steps               │ 1                           │\n",
      "├─────────────────────────────┼─────────────────────────────┤\n",
      "│ report_to                   │ ['tensorboard']             │\n",
      "├─────────────────────────────┼─────────────────────────────┤\n",
      "│ model_max_length            │ 1024                        │\n",
      "├─────────────────────────────┼─────────────────────────────┤\n",
      "│ data_max_length             │ 1024                        │\n",
      "├─────────────────────────────┼─────────────────────────────┤\n",
      "│ use_lora                    │ True                        │\n",
      "├─────────────────────────────┼─────────────────────────────┤\n",
      "│ gradient_checkpointing      │ True                        │\n",
      "├─────────────────────────────┼─────────────────────────────┤\n",
      "│ bf16                        │ True                        │\n",
      "├─────────────────────────────┼─────────────────────────────┤\n",
      "│ target_loss_only            │ True                        │\n",
      "╘═════════════════════════════╧═════════════════════════════╛\n",
      "\n",
      "\n",
      "Arguments table: lora_args\n",
      "╒═════════════════════╤═══════════════════════════════════════════════════════════════════════════════╕\n",
      "│ lora_r              │ 64                                                                            │\n",
      "├─────────────────────┼───────────────────────────────────────────────────────────────────────────────┤\n",
      "│ lora_alpha          │ 16                                                                            │\n",
      "├─────────────────────┼───────────────────────────────────────────────────────────────────────────────┤\n",
      "│ lora_dropout        │ 0.05                                                                          │\n",
      "├─────────────────────┼───────────────────────────────────────────────────────────────────────────────┤\n",
      "│ lora_target_modules │ ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'] │\n",
      "├─────────────────────┼───────────────────────────────────────────────────────────────────────────────┤\n",
      "│ lora_weight_path    │                                                                               │\n",
      "├─────────────────────┼───────────────────────────────────────────────────────────────────────────────┤\n",
      "│ lora_bias           │ none                                                                          │\n",
      "├─────────────────────┼───────────────────────────────────────────────────────────────────────────────┤\n",
      "│ q_lora              │ True                                                                          │\n",
      "╘═════════════════════╧═══════════════════════════════════════════════════════════════════════════════╛\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are calling `save_pretrained` to a 4-bit converted model, but your `bitsandbytes` version doesn't support it. If you want to save 4-bit models, make sure to have `bitsandbytes>=0.41.3` installed.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[32m2024-05-16 11:04:02.473\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mhftrainer.trainer.base\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m171\u001b[0m - \u001b[34m\u001b[1mModel and tokenizer loaded successfully.\u001b[0m\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished getting lengths in 0.00s\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/py39-34/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/envs/py39-34/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "\u001b[32m2024-05-16 11:04:04.176\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfast_hf_llm_trainer.trainer\u001b[0m:\u001b[36mcompute_loss\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1m[RANK=0] Step: 0, Target Loss Only: True, Input Tokens: 811, Target Tokens: 811, Loss: 1.9488, Loss Scale Factor: 0.9473\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='155' max='103302' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   155/103302 00:43 < 8:12:37, 3.49 it/s, Epoch 0.03/18]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-05-16 11:04:07.202\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfast_hf_llm_trainer.trainer\u001b[0m:\u001b[36mcompute_loss\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1m[RANK=0] Step: 10, Target Loss Only: True, Input Tokens: 629, Target Tokens: 629, Loss: 3.1264, Loss Scale Factor: 1.0065\u001b[0m\n",
      "\u001b[32m2024-05-16 11:04:10.296\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfast_hf_llm_trainer.trainer\u001b[0m:\u001b[36mcompute_loss\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1m[RANK=0] Step: 20, Target Loss Only: True, Input Tokens: 669, Target Tokens: 669, Loss: 2.7534, Loss Scale Factor: 1.0065\u001b[0m\n",
      "\u001b[32m2024-05-16 11:04:13.408\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfast_hf_llm_trainer.trainer\u001b[0m:\u001b[36mcompute_loss\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1m[RANK=0] Step: 30, Target Loss Only: True, Input Tokens: 939, Target Tokens: 939, Loss: 2.2891, Loss Scale Factor: 0.9473\u001b[0m\n",
      "\u001b[32m2024-05-16 11:04:16.446\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfast_hf_llm_trainer.trainer\u001b[0m:\u001b[36mcompute_loss\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1m[RANK=0] Step: 40, Target Loss Only: True, Input Tokens: 529, Target Tokens: 529, Loss: 3.5548, Loss Scale Factor: 0.9473\u001b[0m\n",
      "\u001b[32m2024-05-16 11:04:19.447\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfast_hf_llm_trainer.trainer\u001b[0m:\u001b[36mcompute_loss\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1m[RANK=0] Step: 50, Target Loss Only: True, Input Tokens: 611, Target Tokens: 611, Loss: 3.2784, Loss Scale Factor: 1.0065\u001b[0m\n",
      "\u001b[32m2024-05-16 11:04:22.368\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfast_hf_llm_trainer.trainer\u001b[0m:\u001b[36mcompute_loss\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1m[RANK=0] Step: 60, Target Loss Only: True, Input Tokens: 659, Target Tokens: 659, Loss: 3.0942, Loss Scale Factor: 1.0065\u001b[0m\n",
      "\u001b[32m2024-05-16 11:04:25.327\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfast_hf_llm_trainer.trainer\u001b[0m:\u001b[36mcompute_loss\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1m[RANK=0] Step: 70, Target Loss Only: True, Input Tokens: 466, Target Tokens: 466, Loss: 2.9980, Loss Scale Factor: 0.9526\u001b[0m\n",
      "\u001b[32m2024-05-16 11:04:28.109\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfast_hf_llm_trainer.trainer\u001b[0m:\u001b[36mcompute_loss\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1m[RANK=0] Step: 80, Target Loss Only: True, Input Tokens: 585, Target Tokens: 585, Loss: 3.1173, Loss Scale Factor: 1.0716\u001b[0m\n",
      "\u001b[32m2024-05-16 11:04:30.811\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfast_hf_llm_trainer.trainer\u001b[0m:\u001b[36mcompute_loss\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1m[RANK=0] Step: 90, Target Loss Only: True, Input Tokens: 600, Target Tokens: 600, Loss: 3.7182, Loss Scale Factor: 1.0716\u001b[0m\n",
      "\u001b[32m2024-05-16 11:04:33.621\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfast_hf_llm_trainer.trainer\u001b[0m:\u001b[36mcompute_loss\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1m[RANK=0] Step: 100, Target Loss Only: True, Input Tokens: 686, Target Tokens: 686, Loss: 2.6169, Loss Scale Factor: 0.9526\u001b[0m\n",
      "\u001b[32m2024-05-16 11:04:36.494\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfast_hf_llm_trainer.trainer\u001b[0m:\u001b[36mcompute_loss\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1m[RANK=0] Step: 110, Target Loss Only: True, Input Tokens: 582, Target Tokens: 582, Loss: 2.9474, Loss Scale Factor: 1.0121\u001b[0m\n",
      "\u001b[32m2024-05-16 11:04:39.273\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfast_hf_llm_trainer.trainer\u001b[0m:\u001b[36mcompute_loss\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1m[RANK=0] Step: 120, Target Loss Only: True, Input Tokens: 591, Target Tokens: 591, Loss: 3.2226, Loss Scale Factor: 1.0716\u001b[0m\n",
      "\u001b[32m2024-05-16 11:04:42.064\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfast_hf_llm_trainer.trainer\u001b[0m:\u001b[36mcompute_loss\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1m[RANK=0] Step: 130, Target Loss Only: True, Input Tokens: 587, Target Tokens: 587, Loss: 2.2410, Loss Scale Factor: 1.0083\u001b[0m\n",
      "\u001b[32m2024-05-16 11:04:44.834\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfast_hf_llm_trainer.trainer\u001b[0m:\u001b[36mcompute_loss\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1m[RANK=0] Step: 140, Target Loss Only: True, Input Tokens: 467, Target Tokens: 467, Loss: 2.8396, Loss Scale Factor: 1.0083\u001b[0m\n",
      "\u001b[32m2024-05-16 11:04:47.407\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfast_hf_llm_trainer.trainer\u001b[0m:\u001b[36mcompute_loss\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1m[RANK=0] Step: 150, Target Loss Only: True, Input Tokens: 528, Target Tokens: 528, Loss: 2.3177, Loss Scale Factor: 1.0083\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from speedy import *\n",
    "from hftrainer.trainer.datasets import LazySupervisedDataset\n",
    "os.environ[\"JUPYTER\"] = \"True\"\n",
    "from hftrainer.trainer.base import *\n",
    "from speedy import *\n",
    "from fast_hf_llm_trainer import DynamicBatchingTrainer\n",
    "trainer = DynamicBatchingTrainer('hf-trainer-template/config/template_args.yaml')\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
