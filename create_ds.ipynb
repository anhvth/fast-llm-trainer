{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from speedy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-26 17:07:48.344275: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-26 17:07:48.393140: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-26 17:07:49,974] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96633652190049dbb7f93d08d4c8f5ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    ")\n",
    "from trl import ORPOConfig, ORPOTrainer, setup_chat_format\n",
    "\n",
    "# Model\n",
    "base_model = \"/public-llm/Meta-Llama-3-8B-Instruct/\"\n",
    "new_model = \"OrpoLlama-3-8B\"\n",
    "# Defined in the secrets tab in Google Colab\n",
    "# Set torch dtype and attention implementation\n",
    "torch_dtype = torch.bfloat16\n",
    "attn_implementation = \"flash_attention_2\"\n",
    "# QLoRA config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# LoRA config\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    # quantization_config=bnb_config,\n",
    "    device_map='cpu',\n",
    "    # attn_implementation=attn_implementation\n",
    ")\n",
    "model, tokenizer = setup_chat_format(model, tokenizer)\n",
    "# model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from llm_utils import transform_messages, load_chat_dataset\n",
    "from copy import deepcopy\n",
    "from transformers.trainer_pt_utils import LabelSmoother\n",
    "IGNORE_TOKEN_ID = LabelSmoother.ignore_index\n",
    "\n",
    "# TEMPLATE = \"{% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}{% endif %}{{'<|im_start|>' + message['role'] + '\\n' + message['content']}}{% if loop.last %}{{ '<|im_end|>'}}{% else %}{{ '<|im_end|>\\n' }}{% endif %}{% endfor %}\"\n",
    "TEMPLATE = \"{% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{% endif %}{% if loop.last %}{% else %}{% endif %}{% endfor %}\"\n",
    "\n",
    "def format_input_messages(\n",
    "    messages:List[dict[str,str]],\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    "    max_len: int = None,\n",
    "    target_loss_only=False,\n",
    ") -> Dict:\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    assert hasattr(tokenizer, 'pad_token_id'), \"Tokenizer must have pad_token_id attribute\"\n",
    "    input_ids = []\n",
    "    target_ids = []\n",
    "\n",
    "    input_id = []\n",
    "    target_id = []\n",
    "    for i in range(len(messages)):\n",
    "        _ids = tokenizer.apply_chat_template(\n",
    "            [messages[i]], tokenize=True, add_special_tokens=False)\n",
    "        input_id += _ids\n",
    "        if target_loss_only and  messages[i]['role'] == 'assistant':\n",
    "            target_id += _ids\n",
    "        else:\n",
    "            target_id += [IGNORE_TOKEN_ID]*len(_ids)\n",
    "\n",
    "    # maxlen\n",
    "    input_id = input_id[:max_len]\n",
    "    target_id = target_id[:max_len]\n",
    "    # to tensor\n",
    "    input_ids = torch.tensor([input_id], dtype=torch.long)\n",
    "    target_ids = torch.tensor([target_id], dtype=torch.long)\n",
    "    attention_mask = input_ids.ne(tokenizer.pad_token_id)\n",
    "    return dict(\n",
    "        input_ids=input_ids, target_ids=target_ids, attention_mask=attention_mask, labels=target_ids,\n",
    "        length=len(input_ids),\n",
    "        num_train_tokens=target_ids.ne(IGNORE_TOKEN_ID).sum().item(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-04-26 17:10:10.030\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_utils.load_chat_dataset\u001b[0m:\u001b[36mload_chat_dataset\u001b[0m:\u001b[36m12\u001b[0m - \u001b[1mConverting the /anhvth5/data/chat-formated-dataset/dataset_factory/get_oasst2_chatml_13848_samples.json from sharegpt to chatml.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "list_msgs = load_chat_dataset('/anhvth5/data/chat-formated-dataset/dataset_factory/get_oasst2_chatml_13848_samples.json')\n",
    "max_len = 8000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 13848/13848 [00:01<00:00, 13592.36it/s] \n"
     ]
    }
   ],
   "source": [
    "# formated_msg = [format_input_messages(item, tokenizer, max_len) for item in list_msgs]\n",
    "inputs = [[item, tokenizer, max_len] for item in list_msgs]\n",
    "def f(input):\n",
    "    return format_input_messages(*input)\n",
    "f(inputs[0])\n",
    "formated_msg = multi_thread(f, inputs, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from speedy import Clock\n",
    "\n",
    "def group_batches_by_sequence_length(items, bz, shuffle=True, drop_last=True):\n",
    "    batches = []\n",
    "    items = sorted(items, key=lambda x: len(x[0]))\n",
    "    for i in range(0, len(items), bz):\n",
    "        batch = items[i : i + bz]\n",
    "        if len(batch) < bz and drop_last:\n",
    "            continue\n",
    "        batches.append(batch)\n",
    "    if shuffle:\n",
    "        random.shuffle(batches)\n",
    "    # flatten\n",
    "    batches = [item for sublist in batches for item in sublist]\n",
    "    return batches\n",
    "\n",
    "def get_available_sequence_lengths(sorted_lengths, len_left, first_item_len):\n",
    "    cond1 = sorted_lengths <= len_left\n",
    "    cond2 = np.abs(sorted_lengths - first_item_len) < 64 if first_item_len is not None else None\n",
    "    available_lengths = sorted_lengths[np.logical_and(cond1, cond2) if cond2 is not None else cond1]\n",
    "    return available_lengths\n",
    "\n",
    "def update_length_to_indexes_mapping(len_to_indexes, chosen_length, sorted_lengths):\n",
    "    if len(len_to_indexes[chosen_length]) == 0:\n",
    "        del len_to_indexes[chosen_length]\n",
    "        sorted_lengths = sorted_lengths[sorted_lengths != chosen_length]\n",
    "    return len_to_indexes, sorted_lengths\n",
    "\n",
    "def create_batches_with_split_points(item_metas, max_length):\n",
    "    len_to_indexes = {}\n",
    "    for item_meta in item_metas:\n",
    "        length = item_meta[\"length\"]\n",
    "        index = item_meta[\"idx\"]\n",
    "        if item_meta[\"num_train_token\"] == 0:\n",
    "            continue\n",
    "        if length <= max_length:\n",
    "            if length not in len_to_indexes:\n",
    "                len_to_indexes[length] = []\n",
    "            len_to_indexes[length].append(index)\n",
    "\n",
    "    batches_with_split_points = []\n",
    "    sorted_lengths = np.array(sorted(len_to_indexes.keys()))\n",
    "    while len(len_to_indexes) > 0:\n",
    "        current_batch_indexes = []\n",
    "        current_split_points = []\n",
    "        len_left = max_length\n",
    "        accumulated_length = 0\n",
    "        first_item_len = None\n",
    "        max_item_len = 0\n",
    "\n",
    "        while len(len_to_indexes) > 0:\n",
    "            available_lengths = get_available_sequence_lengths(sorted_lengths, len_left, first_item_len)\n",
    "            if len(available_lengths) == 0:\n",
    "                break\n",
    "            chosen_length = random.choice(available_lengths)\n",
    "            _id = np.random.choice(len(len_to_indexes[chosen_length]))\n",
    "            chosen_index = len_to_indexes[chosen_length].pop(_id)\n",
    "            if first_item_len is None:\n",
    "                first_item_len = chosen_length\n",
    "                max_item_len = max(max_item_len, chosen_length)\n",
    "            current_batch_indexes.append(chosen_index)\n",
    "            accumulated_length += chosen_length\n",
    "            current_split_points.append(accumulated_length)\n",
    "            len_left = max_length - max_item_len * len(current_batch_indexes) * 1.2\n",
    "            len_to_indexes, sorted_lengths = update_length_to_indexes_mapping(len_to_indexes, chosen_length, sorted_lengths)\n",
    "\n",
    "        if current_batch_indexes:\n",
    "            batches_with_split_points.append((current_batch_indexes, current_split_points))\n",
    "            first_item_len = None\n",
    "\n",
    "    return batches_with_split_points\n",
    "\n",
    "def create_chunks_with_train_tokens(item_metas, max_length, num_gpus, accumulate_steps, target_loss_only=False):\n",
    "    # check\n",
    "    if target_loss_only:\n",
    "        assert 'num_train_token' in item_metas[0], 'num_train_token is required in item_metas when target_loss_only is True'\n",
    "        idx_to_num_train_tokens = {item[\"idx\"]: item[\"num_train_token\"] for item in item_metas}\n",
    "    elif not 'num_train_token' in item_metas[0]:\n",
    "        logger.warning('num_train_token is not found in item_metas. Using length instead')\n",
    "        idx_to_num_train_tokens = {item[\"idx\"]: item[\"length\"] for item in item_metas}\n",
    "    else:\n",
    "        assert 'num_train_token' in item_metas[0], 'num_train_token is required in item_metas when target_loss_only is False'\n",
    "        idx_to_num_train_tokens = {item[\"idx\"]: item[\"num_train_token\"] for item in item_metas}\n",
    "\n",
    "    batches_with_split_points = create_batches_with_split_points(item_metas, max_length)\n",
    "\n",
    "    batches = []\n",
    "    batches_with_split_points = group_batches_by_sequence_length(\n",
    "        batches_with_split_points, num_gpus, shuffle=True, drop_last=False\n",
    "    )\n",
    "    global_bz = num_gpus * accumulate_steps\n",
    "    for i in range(0, len(batches_with_split_points), global_bz):\n",
    "        global_batch_items = batches_with_split_points[i : i + global_bz]\n",
    "        _all_ids_flat = [item for sublist in global_batch_items for item in sublist[0]]\n",
    "        num_train_tokens_total = sum([idx_to_num_train_tokens[idx] for idx in _all_ids_flat])\n",
    "        avg_train_token_in_this_global_batch = num_train_tokens_total / global_bz\n",
    "        new_data = []\n",
    "        for ids, split_points in global_batch_items:\n",
    "            train_tokens = sum([idx_to_num_train_tokens[idx] for idx in ids])\n",
    "            loss_scale_factor = train_tokens / avg_train_token_in_this_global_batch\n",
    "            new_data.append({'item_ids': ids, 'split_points': split_points, 'loss_scale_factor': loss_scale_factor})\n",
    "        batches.append(new_data)\n",
    "    batches_with_split_points = [item for sublist in batches for item in sublist]\n",
    "    return batches_with_split_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(formated_msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>target_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>labels</th>\n",
       "      <th>length</th>\n",
       "      <th>num_train_tokens</th>\n",
       "      <th>idx</th>\n",
       "      <th>num_train_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7713</th>\n",
       "      <td>[[tensor(128256), tensor(9125), tensor(198), t...</td>\n",
       "      <td>[[tensor(-100), tensor(-100), tensor(-100), te...</td>\n",
       "      <td>[[tensor(True), tensor(True), tensor(True), te...</td>\n",
       "      <td>[[tensor(-100), tensor(-100), tensor(-100), te...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7713</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12715</th>\n",
       "      <td>[[tensor(128256), tensor(9125), tensor(198), t...</td>\n",
       "      <td>[[tensor(-100), tensor(-100), tensor(-100), te...</td>\n",
       "      <td>[[tensor(True), tensor(True), tensor(True), te...</td>\n",
       "      <td>[[tensor(-100), tensor(-100), tensor(-100), te...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12715</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2426</th>\n",
       "      <td>[[tensor(128256), tensor(9125), tensor(198), t...</td>\n",
       "      <td>[[tensor(-100), tensor(-100), tensor(-100), te...</td>\n",
       "      <td>[[tensor(True), tensor(True), tensor(True), te...</td>\n",
       "      <td>[[tensor(-100), tensor(-100), tensor(-100), te...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2426</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12219</th>\n",
       "      <td>[[tensor(128256), tensor(9125), tensor(198), t...</td>\n",
       "      <td>[[tensor(-100), tensor(-100), tensor(-100), te...</td>\n",
       "      <td>[[tensor(True), tensor(True), tensor(True), te...</td>\n",
       "      <td>[[tensor(-100), tensor(-100), tensor(-100), te...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12219</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8864</th>\n",
       "      <td>[[tensor(128256), tensor(9125), tensor(198), t...</td>\n",
       "      <td>[[tensor(-100), tensor(-100), tensor(-100), te...</td>\n",
       "      <td>[[tensor(True), tensor(True), tensor(True), te...</td>\n",
       "      <td>[[tensor(-100), tensor(-100), tensor(-100), te...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8864</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10058</th>\n",
       "      <td>[[tensor(128256), tensor(9125), tensor(198), t...</td>\n",
       "      <td>[[tensor(-100), tensor(-100), tensor(-100), te...</td>\n",
       "      <td>[[tensor(True), tensor(True), tensor(True), te...</td>\n",
       "      <td>[[tensor(-100), tensor(-100), tensor(-100), te...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10058</td>\n",
       "      <td>4380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13775</th>\n",
       "      <td>[[tensor(128256), tensor(9125), tensor(198), t...</td>\n",
       "      <td>[[tensor(-100), tensor(-100), tensor(-100), te...</td>\n",
       "      <td>[[tensor(True), tensor(True), tensor(True), te...</td>\n",
       "      <td>[[tensor(-100), tensor(-100), tensor(-100), te...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13775</td>\n",
       "      <td>4948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13257</th>\n",
       "      <td>[[tensor(128256), tensor(9125), tensor(198), t...</td>\n",
       "      <td>[[tensor(-100), tensor(-100), tensor(-100), te...</td>\n",
       "      <td>[[tensor(True), tensor(True), tensor(True), te...</td>\n",
       "      <td>[[tensor(-100), tensor(-100), tensor(-100), te...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13257</td>\n",
       "      <td>4973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1248</th>\n",
       "      <td>[[tensor(128256), tensor(9125), tensor(198), t...</td>\n",
       "      <td>[[tensor(-100), tensor(-100), tensor(-100), te...</td>\n",
       "      <td>[[tensor(True), tensor(True), tensor(True), te...</td>\n",
       "      <td>[[tensor(-100), tensor(-100), tensor(-100), te...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1248</td>\n",
       "      <td>5062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>841</th>\n",
       "      <td>[[tensor(128256), tensor(9125), tensor(198), t...</td>\n",
       "      <td>[[tensor(-100), tensor(-100), tensor(-100), te...</td>\n",
       "      <td>[[tensor(True), tensor(True), tensor(True), te...</td>\n",
       "      <td>[[tensor(-100), tensor(-100), tensor(-100), te...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>841</td>\n",
       "      <td>5221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13848 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input_ids  \\\n",
       "7713   [[tensor(128256), tensor(9125), tensor(198), t...   \n",
       "12715  [[tensor(128256), tensor(9125), tensor(198), t...   \n",
       "2426   [[tensor(128256), tensor(9125), tensor(198), t...   \n",
       "12219  [[tensor(128256), tensor(9125), tensor(198), t...   \n",
       "8864   [[tensor(128256), tensor(9125), tensor(198), t...   \n",
       "...                                                  ...   \n",
       "10058  [[tensor(128256), tensor(9125), tensor(198), t...   \n",
       "13775  [[tensor(128256), tensor(9125), tensor(198), t...   \n",
       "13257  [[tensor(128256), tensor(9125), tensor(198), t...   \n",
       "1248   [[tensor(128256), tensor(9125), tensor(198), t...   \n",
       "841    [[tensor(128256), tensor(9125), tensor(198), t...   \n",
       "\n",
       "                                              target_ids  \\\n",
       "7713   [[tensor(-100), tensor(-100), tensor(-100), te...   \n",
       "12715  [[tensor(-100), tensor(-100), tensor(-100), te...   \n",
       "2426   [[tensor(-100), tensor(-100), tensor(-100), te...   \n",
       "12219  [[tensor(-100), tensor(-100), tensor(-100), te...   \n",
       "8864   [[tensor(-100), tensor(-100), tensor(-100), te...   \n",
       "...                                                  ...   \n",
       "10058  [[tensor(-100), tensor(-100), tensor(-100), te...   \n",
       "13775  [[tensor(-100), tensor(-100), tensor(-100), te...   \n",
       "13257  [[tensor(-100), tensor(-100), tensor(-100), te...   \n",
       "1248   [[tensor(-100), tensor(-100), tensor(-100), te...   \n",
       "841    [[tensor(-100), tensor(-100), tensor(-100), te...   \n",
       "\n",
       "                                          attention_mask  \\\n",
       "7713   [[tensor(True), tensor(True), tensor(True), te...   \n",
       "12715  [[tensor(True), tensor(True), tensor(True), te...   \n",
       "2426   [[tensor(True), tensor(True), tensor(True), te...   \n",
       "12219  [[tensor(True), tensor(True), tensor(True), te...   \n",
       "8864   [[tensor(True), tensor(True), tensor(True), te...   \n",
       "...                                                  ...   \n",
       "10058  [[tensor(True), tensor(True), tensor(True), te...   \n",
       "13775  [[tensor(True), tensor(True), tensor(True), te...   \n",
       "13257  [[tensor(True), tensor(True), tensor(True), te...   \n",
       "1248   [[tensor(True), tensor(True), tensor(True), te...   \n",
       "841    [[tensor(True), tensor(True), tensor(True), te...   \n",
       "\n",
       "                                                  labels  length  \\\n",
       "7713   [[tensor(-100), tensor(-100), tensor(-100), te...       1   \n",
       "12715  [[tensor(-100), tensor(-100), tensor(-100), te...       1   \n",
       "2426   [[tensor(-100), tensor(-100), tensor(-100), te...       1   \n",
       "12219  [[tensor(-100), tensor(-100), tensor(-100), te...       1   \n",
       "8864   [[tensor(-100), tensor(-100), tensor(-100), te...       1   \n",
       "...                                                  ...     ...   \n",
       "10058  [[tensor(-100), tensor(-100), tensor(-100), te...       1   \n",
       "13775  [[tensor(-100), tensor(-100), tensor(-100), te...       1   \n",
       "13257  [[tensor(-100), tensor(-100), tensor(-100), te...       1   \n",
       "1248   [[tensor(-100), tensor(-100), tensor(-100), te...       1   \n",
       "841    [[tensor(-100), tensor(-100), tensor(-100), te...       1   \n",
       "\n",
       "       num_train_tokens    idx  num_train_token  \n",
       "7713                  0   7713               27  \n",
       "12715                 0  12715               27  \n",
       "2426                  0   2426               28  \n",
       "12219                 0  12219               29  \n",
       "8864                  0   8864               30  \n",
       "...                 ...    ...              ...  \n",
       "10058                 0  10058             4380  \n",
       "13775                 0  13775             4948  \n",
       "13257                 0  13257             4973  \n",
       "1248                  0   1248             5062  \n",
       "841                   0    841             5221  \n",
       "\n",
       "[13848 rows x 8 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tokenizer.decode(df.iloc[506]['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
